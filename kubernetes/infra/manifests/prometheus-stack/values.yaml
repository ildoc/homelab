---
# kubernetes/infra/manifests/prometheus-stack/values.yaml
# Configurazione pulita basata sui default del chart ufficiale

kube-prometheus-stack:
  ## Configurazione Prometheus Operator
  prometheusOperator:
    admissionWebhooks:
      enabled: false  # Disabilitato per semplificare con ArgoCD
    
    # Fix per il problema TLS
    tls:
      enabled: false  # Disabilita TLS per evitare il problema del secret mancante
    
    resources:
      limits:
        cpu: 200m
        memory: 200Mi
      requests:
        cpu: 100m
        memory: 100Mi

  ## Configurazione Prometheus
  prometheus:
    prometheusSpec:
      replicas: 2
      retention: 30d
      retentionSize: 15GB
      
      resources:
        requests:
          cpu: 250m
          memory: 1Gi
        limits:
          cpu: 1000m
          memory: 2Gi
      
      storageSpec:
        volumeClaimTemplate:
          spec:
            storageClassName: nfs-storage
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 20Gi
      
      # Disabilita i selettori basati su Helm per avere controllo completo
      serviceMonitorSelectorNilUsesHelmValues: false
      podMonitorSelectorNilUsesHelmValues: false
      ruleSelectorNilUsesHelmValues: false
      probeSelectorNilUsesHelmValues: false
      
      externalLabels:
        cluster: homelab
      
      # Scrape configs per GitLab e servizi esterni
      additionalScrapeConfigs:
        - job_name: 'gitlab-omnibus'
          static_configs:
            - targets: ['gitlab.local.ildoc.it:9090']
          relabel_configs:
            - source_labels: [__address__]
              target_label: instance
              replacement: gitlab
        
        - job_name: 'gitlab-exporter'
          static_configs:
            - targets: ['gitlab.local.ildoc.it:9168']
          metrics_path: /metrics
          relabel_configs:
            - source_labels: [__address__]
              target_label: instance
              replacement: gitlab
        
        - job_name: 'gitlab-sidekiq'
          static_configs:
            - targets: ['gitlab.local.ildoc.it:8082']
          relabel_configs:
            - source_labels: [__address__]
              target_label: instance
              replacement: gitlab-sidekiq
        
        - job_name: 'gitlab-workhorse'
          static_configs:
            - targets: ['gitlab.local.ildoc.it:9229']
          relabel_configs:
            - source_labels: [__address__]
              target_label: instance
              replacement: gitlab-workhorse

  ## Configurazione Grafana
  grafana:
    enabled: true
    
    # Admin credentials from External Secrets
    admin:
      existingSecret: grafana-admin-secret
      userKey: admin-user
      passwordKey: admin-password
    
    # Persistence
    persistence:
      enabled: true
      storageClassName: nfs-storage
      size: 5Gi
      accessModes:
        - ReadWriteOnce
    
    # Resources
    resources:
      limits:
        cpu: 200m
        memory: 256Mi
      requests:
        cpu: 100m
        memory: 128Mi
    
    # Grafana configuration
    grafana.ini:
      server:
        domain: grafana.local.ildoc.it
        root_url: https://grafana.local.ildoc.it
        serve_from_sub_path: false
      auth.anonymous:
        enabled: true
        org_role: Viewer
      security:
        allow_embedding: true
    
    # Sidecar for dashboards and datasources
    sidecar:
      dashboards:
        enabled: true
        label: grafana_dashboard
        labelValue: "1"
        searchNamespace: ALL
        provider:
          allowUiUpdates: true
      datasources:
        enabled: true
        defaultDatasourceEnabled: true
        isDefaultDatasource: true
        uid: prometheus
        name: Prometheus
        type: prometheus
        url: http://prometheus-stack-kube-prome-prometheus:9090
        access: proxy
    
    # Dashboard configuration - usando il metodo standard del chart
    dashboardProviders:
      dashboardproviders.yaml:
        apiVersion: 1
        providers:
          - name: 'default'
            orgId: 1
            folder: ''
            type: file
            disableDeletion: false
            updateIntervalSeconds: 10
            allowUiUpdates: true
            options:
              path: /var/lib/grafana/dashboards/default
    
    # Pre-configured dashboards from grafana.com
    dashboards:
      default:
        kubernetes-cluster:
          gnetId: 7249
          revision: 1
          datasource: Prometheus
        node-exporter:
          gnetId: 1860
          revision: 37
          datasource: Prometheus
        proxmox:
          gnetId: 10347
          revision: 5
          datasource: Prometheus
        gitlab-overview:
          gnetId: 10902
          revision: 1
          datasource: Prometheus
        traefik:
          gnetId: 17347
          revision: 9
          datasource: Prometheus

  ## Configurazione AlertManager
  alertmanager:
    alertmanagerSpec:
      replicas: 1
      retention: 120h
      
      storage:
        volumeClaimTemplate:
          spec:
            storageClassName: nfs-storage
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 1Gi
      
      resources:
        requests:
          cpu: 50m
          memory: 64Mi
        limits:
          cpu: 100m
          memory: 128Mi
      
      # Mount secrets for telegram credentials
      secrets:
        - alertmanager-telegram-secret
    
    # Configurazione AlertManager
    config:
      global:
        resolve_timeout: 5m
      
      route:
        group_by: ['alertname', 'cluster', 'service']
        group_wait: 10s
        group_interval: 10s
        repeat_interval: 12h
        receiver: 'default'
        routes:
          - match:
              severity: critical
            receiver: 'telegram'
            repeat_interval: 1h
          - match:
              severity: warning
            receiver: 'telegram'
            repeat_interval: 4h
          - match:
              alertname: Watchdog
            receiver: 'null'
      
      receivers:
        - name: 'null'
        
        - name: 'default'
          telegram_configs:
            - bot_token_file: '/etc/alertmanager/secrets/alertmanager-telegram-secret/telegram-bot-token'
              chat_id_file: '/etc/alertmanager/secrets/alertmanager-telegram-secret/telegram-chat-id'
              api_url: 'https://api.telegram.org'
              parse_mode: 'HTML'
              message: |
                üö® <b>{{ .GroupLabels.alertname }}</b>
                <b>Severity:</b> {{ .CommonLabels.severity }}
                {{ range .Alerts }}
                üìç <b>{{ .Labels.instance | reReplaceAll ":.*" "" }}</b>
                {{ .Annotations.summary }}
                {{ end }}
        
        - name: 'telegram'
          telegram_configs:
            - bot_token_file: '/etc/alertmanager/secrets/alertmanager-telegram-secret/telegram-bot-token'
              chat_id_file: '/etc/alertmanager/secrets/alertmanager-telegram-secret/telegram-chat-id'
              api_url: 'https://api.telegram.org'
              parse_mode: 'HTML'
              send_resolved: true
              message: |
                {{ if eq .Status "firing" }}üî¥{{ else }}‚úÖ{{ end }} <b>{{ .GroupLabels.alertname }}</b>
                <b>Status:</b> {{ .Status }}
                {{ range .Alerts }}
                ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
                üìç <b>Instance:</b> {{ .Labels.instance | reReplaceAll ":.*" "" }}
                {{ if .Annotations.summary }}<b>Summary:</b> {{ .Annotations.summary }}{{ end }}
                {{ end }}
      
      inhibit_rules:
        - source_match:
            severity: 'critical'
          target_match:
            severity: 'warning'
          equal:
            - 'alertname'
            - 'instance'

  ## Componenti aggiuntivi
  nodeExporter:
    enabled: true
  
  kubeStateMetrics:
    enabled: true
  
  ## Disabilita componenti che potrebbero non essere accessibili
  kubeControllerManager:
    enabled: false
  kubeEtcd:
    enabled: false
  kubeScheduler:
    enabled: false

## Configurazioni custom per il monitoring aggiuntivo
## Queste sono utilizzate dai template custom
monitoring:
  # Configurazione HTTPRoutes
  routes:
    enabled: true
    gateway:
      name: "traefik-gateway"
      namespace: "traefik"
    grafana:
      enabled: true
      hostname: "grafana.local.ildoc.it"
    prometheus:
      enabled: true
      hostname: "prometheus.local.ildoc.it"
    alertmanager:
      enabled: true
      hostname: "alertmanager.local.ildoc.it"
  
  # Configurazione Probes per Blackbox Exporter
  probes:
    enabled: true
    interval: "60s"
    module: "http_2xx"
    environment: "homelab"
    targets:
      # Servizi infrastrutturali
      - "https://grafana.local.ildoc.it"
      - "https://argocd.local.ildoc.it"
      - "https://rancher.local.ildoc.it"
      - "https://gitlab.local.ildoc.it"
      # Applicazioni
      - "https://audiobookshelf.local.ildoc.it"
      - "https://bazarr.local.ildoc.it"
      - "https://mealie.local.ildoc.it"
      - "https://n8n.local.ildoc.it"
      - "https://speedtest.local.ildoc.it"
      - "https://tools.local.ildoc.it"
      - "https://uptime.local.ildoc.it"
      - "https://pdf.local.ildoc.it"
      - "https://nut.local.ildoc.it"
  
  # ServiceMonitors per servizi interni
  serviceMonitors:
    traefik:
      enabled: true
      namespace: "traefik"
      selector: "traefik"
      endpoints:
        - port: "metrics"
          interval: "30s"
    
    argocd:
      enabled: true
      namespace: "argocd"
      selector: "argocd-metrics"
      endpoints:
        - port: "metrics"
          interval: "30s"
  
  # Proxmox VE Exporter (opzionale)
  pveExporter:
    enabled: true
    image: "prompve/prometheus-pve-exporter:3.4.5"
    replicas: 1
    verifySsl: false
    scrapeInterval: "30s"
    # Configurazione corretta per pve-exporter
    targets:
      - name: "proxmox-node1"
        address: "192.168.0.10"
        port: 8006
      - name: "proxmox-node2"
        address: "192.168.0.11"
        port: 8006
      - name: "proxmox-node3"
        address: "192.168.0.12"
        port: 8006
    resources:
      limits:
        cpu: "100m"
        memory: "128Mi"
      requests:
        cpu: "50m"
        memory: "64Mi"
